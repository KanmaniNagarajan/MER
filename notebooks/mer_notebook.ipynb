{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MER: Multimodal Emotion Recognition\n",
    "\n",
    "This notebook runs the entire process of training and evaluating a multimodal emotion recognition model using the eNTERFACE'05 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install librosa opencv-python-headless tqdm scikit-learn matplotlib seaborn torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the project repository\n",
    "!git clone https://github.com/your_username/MER.git\n",
    "%cd MER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the test flow\n",
    "!python src/test_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Flow Results\n",
    "\n",
    "Review the output above to ensure that the entire pipeline is working correctly. If everything looks good, proceed with the full training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the full training process\n",
    "!python src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the evaluation process\n",
    "!python src/evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The training logs, confusion matrix, ROC-AUC curve, and other metrics can be found in the `results` folder in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interactive Test Widget\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from src.model import EmotionRecognitionModel\n",
    "from src.config import DEVICE, EMOTIONS, RESULTS_DIR\n",
    "import os\n",
    "\n",
    "model = EmotionRecognitionModel().to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'best_model.pth')))\n",
    "model.eval()\n",
    "\n",
    "def predict_emotion(video_path, audio_path):\n",
    "    # This is a placeholder. You need to implement proper video and audio loading here.\n",
    "    video = torch.randn(1, 3, 16, 112, 112).to(DEVICE)  # Assuming 16 frames of 112x112 RGB images\n",
    "    audio = torch.randn(1, 1, 16000).to(DEVICE)  # Assuming 1 second of audio at 16kHz\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(audio, video)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return EMOTIONS[predicted.item()]\n",
    "\n",
    "video_path = widgets.Text(description='Video Path:')\n",
    "audio_path = widgets.Text(description='Audio Path:')\n",
    "predict_button = widgets.Button(description='Predict')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(_):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Predicted emotion: {predict_emotion(video_path.value, audio_path.value)}\")\n",
    "\n",
    "predict_button.on_click(on_button_clicked)\n",
    "\n",
    "display(video_path, audio_path, predict_button, output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
